{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e661d26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MyPc\\anaconda3\\envs\\ML_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MyPc\\anaconda3\\envs\\ML_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The parameter 'aux_logits' expected value True but got False instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 153\u001b[0m\n\u001b[0;32m    146\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m get_data(\n\u001b[0;32m    147\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m    148\u001b[0m     test_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m    149\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    150\u001b[0m )\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# b) build feature extractor\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m feat_ext \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_googlenet_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# c) extract embeddings on test set\u001b[39;00m\n\u001b[0;32m    156\u001b[0m test_embs \u001b[38;5;241m=\u001b[39m extract_embeddings(test_loader, feat_ext, device)\n",
      "Cell \u001b[1;32mIn[10], line 92\u001b[0m, in \u001b[0;36mbuild_googlenet_extractor\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_googlenet_extractor\u001b[39m(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 92\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgooglenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43maux_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mtransform_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# drop final FC layer\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     feat_ext \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mchildren())[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\MyPc\\anaconda3\\envs\\ML_env\\lib\\site-packages\\torchvision\\models\\_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs\u001b[38;5;241m.\u001b[39mkeys()),\u001b[38;5;250m \u001b[39mseparate_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as positional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    140\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MyPc\\anaconda3\\envs\\ML_env\\lib\\site-packages\\torchvision\\models\\_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[0;32m    226\u001b[0m     kwargs[weights_param] \u001b[38;5;241m=\u001b[39m default_weights_arg\n\u001b[1;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MyPc\\anaconda3\\envs\\ML_env\\lib\\site-packages\\torchvision\\models\\googlenet.py:328\u001b[0m, in \u001b[0;36mgooglenet\u001b[1;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform_input\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m    327\u001b[0m     _ovewrite_named_param(kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform_input\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 328\u001b[0m \u001b[43m_ovewrite_named_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maux_logits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m _ovewrite_named_param(kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    330\u001b[0m _ovewrite_named_param(kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(weights\u001b[38;5;241m.\u001b[39mmeta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\MyPc\\anaconda3\\envs\\ML_env\\lib\\site-packages\\torchvision\\models\\_utils.py:238\u001b[0m, in \u001b[0;36m_ovewrite_named_param\u001b[1;34m(kwargs, param, new_value)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs[param] \u001b[38;5;241m!=\u001b[39m new_value:\n\u001b[1;32m--> 238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe parameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m expected value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[param]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     kwargs[param] \u001b[38;5;241m=\u001b[39m new_value\n",
      "\u001b[1;31mValueError\u001b[0m: The parameter 'aux_logits' expected value True but got False instead."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms \n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import sklearn\n",
    "import numpy\n",
    "import pandas\n",
    "import matplotlib\n",
    "import os\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Paths\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "if os.path.isdir(os.path.join(cwd, 'data', 'training')):\n",
    "    project_root = cwd\n",
    "elif os.path.isdir(os.path.join(cwd, '..', 'data', 'training')):\n",
    "    project_root = os.path.abspath(os.path.join(cwd, '..'))\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        \"Non riesco a trovare la cartella 'data/training' né in \"\n",
    "        f\"{cwd} né in {os.path.abspath(os.path.join(cwd,'..'))}\"\n",
    "    )\n",
    "\n",
    "root_dir_train = os.path.join(project_root, 'data', 'training')\n",
    "root_dir_test  = os.path.join(project_root, 'data', 'test')\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Data loader: train / val / test\n",
    "# -------------------------------------------------------------------\n",
    "def get_data(\n",
    "    batch_size,\n",
    "    test_batch_size=16,\n",
    "    num_workers=2,\n",
    "    mean=None,\n",
    "    std=None,\n",
    "    num_train_samples=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      train_loader, val_loader, test_loader\n",
    "    Computes mean/std on the training set if not supplied.\n",
    "    \"\"\"\n",
    "    target_size = (224, 224)\n",
    "\n",
    "    # 1a) compute mean/std if needed\n",
    "    if mean is None or std is None:\n",
    "        tmp_tf = transforms.Compose([\n",
    "            transforms.Resize(target_size),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        tmp_ds = ImageFolder(root_dir_train, transform=tmp_tf)\n",
    "        imgs = torch.stack([img for img, _ in tmp_ds], dim=0)\n",
    "        mean = float(imgs.mean())\n",
    "        std  = float(imgs.std())\n",
    "\n",
    "    # 1b) full transforms\n",
    "    tf = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[mean], std=[std])\n",
    "    ])\n",
    "\n",
    "    # 1c) datasets\n",
    "    full_train_ds = ImageFolder(root_dir_train, transform=tf)\n",
    "    test_ds       = ImageFolder(root_dir_test,  transform=tf)\n",
    "\n",
    "    # 1d) train / val split\n",
    "    N = len(full_train_ds)\n",
    "    train_N = (int(N * 0.7) if num_train_samples is None else num_train_samples)\n",
    "    val_N   = N - train_N\n",
    "    train_ds, val_ds = torch.utils.data.random_split(full_train_ds, [train_N, val_N])\n",
    "\n",
    "    # 1e) loaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size,       shuffle=True,  num_workers=num_workers)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=test_batch_size,  shuffle=False, num_workers=num_workers)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=test_batch_size,  shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Build GoogLeNet feature extractor\n",
    "# -------------------------------------------------------------------\n",
    "def build_googlenet_extractor(device='cuda'):\n",
    "    model = torchvision.models.googlenet(pretrained=True,\n",
    "                                         aux_logits=False,\n",
    "                                         transform_input=False)\n",
    "    # drop final FC layer\n",
    "    feat_ext = torch.nn.Sequential(*list(model.children())[:-1]).to(device)\n",
    "    feat_ext.eval()\n",
    "    return feat_ext\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) Extract embeddings from a loader\n",
    "# -------------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def extract_embeddings(loader, model, device='cuda'):\n",
    "    embs = []\n",
    "    for imgs, _ in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        feats = model(imgs)               # [B, 1024, 1, 1]\n",
    "        feats = feats.view(feats.size(0), -1)\n",
    "        embs.append(feats.cpu())\n",
    "    return torch.cat(embs, dim=0)         # [N, 1024]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) Top-k retrieval within the same set\n",
    "# -------------------------------------------------------------------\n",
    "def retrieve_topk(query_embs, gallery_embs, query_paths, gallery_paths, k=5):\n",
    "    # normalize for cosine\n",
    "    query_embs   = F.normalize(query_embs, dim=1)\n",
    "    gallery_embs = F.normalize(gallery_embs, dim=1)\n",
    "\n",
    "    # similarity matrix\n",
    "    sim = query_embs @ gallery_embs.t()      # [Q, G]\n",
    "\n",
    "    # get k+1 highest so we can skip self-match at index 0\n",
    "    topk = sim.topk(k + 1, dim=1, largest=True)[1]  # [Q, k+1]\n",
    "\n",
    "    results = []\n",
    "    for qi, idxs in enumerate(topk):\n",
    "        # drop the first idx (self)\n",
    "        neigh = idxs.tolist()[1 : k+1]\n",
    "        qpath = query_paths[qi][0]\n",
    "        retrieved = [gallery_paths[i][0] for i in neigh]\n",
    "        results.append({\n",
    "            'query':    os.path.basename(qpath),\n",
    "            'retrieved': [os.path.basename(p) for p in retrieved]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5) Main script\n",
    "# -------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # a) load data\n",
    "    train_loader, val_loader, test_loader = get_data(\n",
    "        batch_size=16,\n",
    "        test_batch_size=32,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    # b) build feature extractor\n",
    "    feat_ext = build_googlenet_extractor(device)\n",
    "\n",
    "    # c) extract embeddings on test set\n",
    "    test_embs = extract_embeddings(test_loader, feat_ext, device)\n",
    "\n",
    "    # d) get file paths for retrieval\n",
    "    test_paths = test_loader.dataset.imgs    # list of (filepath, class_idx)\n",
    "\n",
    "    # e) retrieve top-10 similar images within the test set\n",
    "    topk_results = retrieve_topk(\n",
    "        test_embs, test_embs,\n",
    "        test_paths, test_paths,\n",
    "        k=10\n",
    "    )\n",
    "\n",
    "    # f) output JSON\n",
    "    import json\n",
    "    print(json.dumps(topk_results, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
