{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e661d26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 1/1 [00:07<00:00,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"query\": \"n01855672_1037.jpg\",\n",
      "    \"retrieved\": [\n",
      "      \"n01855672_4393.jpg\",\n",
      "      \"n01855672_10973.jpg\",\n",
      "      \"n01855672_4197.jpg\",\n",
      "      \"painting_085_000084.jpg\",\n",
      "      \"painting_085_000118.jpg\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"query\": \"n01855672_4197.jpg\",\n",
      "    \"retrieved\": [\n",
      "      \"n01855672_10973.jpg\",\n",
      "      \"n01855672_4393.jpg\",\n",
      "      \"painting_085_000118.jpg\",\n",
      "      \"n01855672_1037.jpg\",\n",
      "      \"painting_085_000084.jpg\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"query\": \"n01855672_4393.jpg\",\n",
      "    \"retrieved\": [\n",
      "      \"n01855672_1037.jpg\",\n",
      "      \"n01855672_4197.jpg\",\n",
      "      \"n01855672_10973.jpg\",\n",
      "      \"painting_085_000084.jpg\",\n",
      "      \"painting_085_000118.jpg\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"query\": \"painting_085_000045.jpg\",\n",
      "    \"retrieved\": [\n",
      "      \"painting_085_000118.jpg\",\n",
      "      \"4597118805213184.jpg\",\n",
      "      \"painting_085_000084.jpg\",\n",
      "      \"n01855672_4197.jpg\",\n",
      "      \"n01855672_4393.jpg\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"query\": \"painting_085_000084.jpg\",\n",
      "    \"retrieved\": [\n",
      "      \"painting_085_000118.jpg\",\n",
      "      \"n01855672_10973.jpg\",\n",
      "      \"painting_085_000045.jpg\",\n",
      "      \"n01855672_4197.jpg\",\n",
      "      \"n01855672_4393.jpg\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"query\": \"painting_085_000118.jpg\",\n",
      "    \"retrieved\": [\n",
      "      \"painting_085_000045.jpg\",\n",
      "      \"painting_085_000084.jpg\",\n",
      "      \"n01855672_4197.jpg\",\n",
      "      \"n01855672_4393.jpg\",\n",
      "      \"4597118805213184.jpg\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"query\": \"4597118805213184.jpg\",\n",
      "    \"retrieved\": [\n",
      "      \"painting_085_000045.jpg\",\n",
      "      \"painting_085_000118.jpg\",\n",
      "      \"painting_085_000084.jpg\",\n",
      "      \"n01855672_10973.jpg\",\n",
      "      \"n01855672_4393.jpg\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"query\": \"n01855672_10973.jpg\",\n",
      "    \"retrieved\": [\n",
      "      \"n01855672_4197.jpg\",\n",
      "      \"n01855672_4393.jpg\",\n",
      "      \"n01855672_1037.jpg\",\n",
      "      \"painting_085_000084.jpg\",\n",
      "      \"painting_085_000118.jpg\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Paths\n",
    "# -------------------------------------------------------------------\n",
    "cwd = os.getcwd()\n",
    "if os.path.isdir(os.path.join(cwd, 'data', 'training')):\n",
    "    project_root = cwd\n",
    "elif os.path.isdir(os.path.join(cwd, '..', 'data', 'training')):\n",
    "    project_root = os.path.abspath(os.path.join(cwd, '..'))\n",
    "else:\n",
    "    raise RuntimeError(\"Impossibile trovare la cartella 'data/training'.\")\n",
    "\n",
    "root_dir_train = os.path.join(project_root, 'data', 'training')\n",
    "root_dir_test  = os.path.join(project_root, 'data', 'test')\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Data loader\n",
    "# -------------------------------------------------------------------\n",
    "def get_data(batch_size, test_batch_size=16, num_workers=2, mean=None, std=None, num_train_samples=None):\n",
    "    target_size = (224, 224)\n",
    "\n",
    "    # 1a) compute mean/std if needed\n",
    "    if mean is None or std is None:\n",
    "        tmp_tf = transforms.Compose([\n",
    "            transforms.Resize(target_size),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        tmp_ds = ImageFolder(root_dir_train, transform=tmp_tf)\n",
    "        imgs = torch.stack([img for img, _ in tmp_ds], dim=0)\n",
    "        mean = float(imgs.mean())\n",
    "        std  = float(imgs.std())\n",
    "\n",
    "    # 1b) transforms\n",
    "    tf = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[mean]*3, std=[std]*3)  # 3 canali RGB\n",
    "    ])\n",
    "\n",
    "    # 1c) datasets\n",
    "    full_train_ds = ImageFolder(root_dir_train, transform=tf)\n",
    "    test_ds       = ImageFolder(root_dir_test,  transform=tf)\n",
    "\n",
    "    # 1d) split\n",
    "    N = len(full_train_ds)\n",
    "    train_N = int(N * 0.7) if num_train_samples is None else num_train_samples\n",
    "    val_N   = N - train_N\n",
    "    train_ds, val_ds = torch.utils.data.random_split(full_train_ds, [train_N, val_N])\n",
    "\n",
    "    # 1e) loaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=num_workers)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=test_batch_size, shuffle=False, num_workers=num_workers)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=test_batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Build GoogLeNet feature extractor\n",
    "# -------------------------------------------------------------------\n",
    "def build_googlenet_extractor(device='cuda'):\n",
    "    weights = torchvision.models.GoogLeNet_Weights.DEFAULT\n",
    "    model = torchvision.models.googlenet(weights=weights, aux_logits=True)  # <-- obbligatorio\n",
    "    model.aux1 = torch.nn.Identity()   # disattiva classificatore ausiliario 1\n",
    "    model.aux2 = torch.nn.Identity()   # disattiva classificatore ausiliario 2\n",
    "    model.fc = torch.nn.Identity()     # ottieni solo il vettore 1024-d\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) Extract embeddings\n",
    "# -------------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def extract_embeddings(loader, model, device='cuda'):\n",
    "    embs = []\n",
    "    for imgs, _ in tqdm(loader, desc=\"Extracting embeddings\"):\n",
    "        imgs = imgs.to(device)\n",
    "        feats = model(imgs)  # [B, 1024]\n",
    "        embs.append(feats.cpu())\n",
    "    return torch.cat(embs, dim=0)  # [N, 1024]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) Top-k retrieval\n",
    "# -------------------------------------------------------------------\n",
    "def retrieve_topk(query_embs, gallery_embs, query_paths, gallery_paths, k=5):\n",
    "    query_embs   = F.normalize(query_embs, dim=1)\n",
    "    gallery_embs = F.normalize(gallery_embs, dim=1)\n",
    "    sim = query_embs @ gallery_embs.t()  # cosine similarity\n",
    "    topk = sim.topk(k + 1, dim=1, largest=True)[1]\n",
    "\n",
    "    results = []\n",
    "    for qi, idxs in enumerate(topk):\n",
    "        neigh = idxs.tolist()[1 : k+1]\n",
    "        qpath = query_paths[qi][0]\n",
    "        retrieved = [gallery_paths[i][0] for i in neigh]\n",
    "        results.append({\n",
    "            'query':    os.path.basename(qpath),\n",
    "            'retrieved': [os.path.basename(p) for p in retrieved]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5) Main script\n",
    "# -------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # a) carica i dati\n",
    "    train_loader, val_loader, test_loader = get_data(\n",
    "        batch_size=16,\n",
    "        test_batch_size=32,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    # b) costruisci il feature extractor\n",
    "    feat_ext = build_googlenet_extractor(device)\n",
    "\n",
    "    # c) estrai embeddings\n",
    "    test_embs = extract_embeddings(test_loader, feat_ext, device)\n",
    "\n",
    "    # d) ottieni i path\n",
    "    test_paths = test_loader.dataset.imgs\n",
    "\n",
    "    # e) retrieval top-k\n",
    "    topk_results = retrieve_topk(test_embs, test_embs, test_paths, test_paths, k=5)\n",
    "\n",
    "    # f) stampa risultati\n",
    "    print(json.dumps(topk_results, indent=2))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
