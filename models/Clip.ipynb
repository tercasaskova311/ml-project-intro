{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9189a654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2700f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, root_folder, transform):\n",
    "        self.transform = transform\n",
    "        self.img_paths = []\n",
    "        self.labels = []\n",
    "        # Walk through subfolders\n",
    "        for label_name in os.listdir(root_folder):\n",
    "            label_path = os.path.join(root_folder, label_name)\n",
    "            if os.path.isdir(label_path):\n",
    "                for img_file in os.listdir(label_path):\n",
    "                    self.img_paths.append(os.path.join(label_path, img_file))\n",
    "                    self.labels.append(label_name)  # folder name as label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.img_paths[idx]).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return image, label, os.path.basename(self.img_paths[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9368e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "836fc1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_clip(train_loader, model, epochs=3, lr=1e-5):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for images, labels, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Prepare text tokens for labels\n",
    "            texts = clip.tokenize(labels).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            image_features = model.encode_image(images)\n",
    "            text_features = model.encode_text(texts)\n",
    "            \n",
    "            # Normalize embeddings\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "            \n",
    "            # Compute logits: image-text similarity\n",
    "            logits_per_image = image_features @ text_features.t()\n",
    "            logits_per_text = text_features @ image_features.t()\n",
    "            \n",
    "            # Labels are diagonal (correct pairs)\n",
    "            ground_truth = torch.arange(len(images), device=device)\n",
    "\n",
    "            # Compute contrastive loss (sum of two cross-entropy losses)\n",
    "            loss = (loss_fn(logits_per_image, ground_truth) + loss_fn(logits_per_text, ground_truth)) / 2\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07026c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(image_folder, model, preprocess):\n",
    "    image_paths = sorted([os.path.join(image_folder, f) for f in os.listdir(image_folder)])\n",
    "    images = [preprocess(Image.open(p).convert(\"RGB\")) for p in image_paths]\n",
    "    images = torch.stack(images).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = model.encode_image(images)\n",
    "        features /= features.norm(dim=-1, keepdim=True)\n",
    "    return features, image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fa8e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query_features, gallery_features, gallery_paths, top_k=5):\n",
    "    similarities = query_features @ gallery_features.T  # cosine similarity matrix\n",
    "    topk_values, topk_indices = similarities.topk(top_k, dim=-1)\n",
    "\n",
    "    results = []\n",
    "    for i in range(query_features.shape[0]):\n",
    "        retrieved = [gallery_paths[idx] for idx in topk_indices[i].cpu().numpy()]\n",
    "        results.append(retrieved)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f6d06dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_retrieval(query_paths, retrieval_results):\n",
    "    for i, query_path in enumerate(query_paths):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Show query\n",
    "        plt.subplot(1, 6, 1)\n",
    "        plt.title(\"Query\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(Image.open(query_path))\n",
    "        \n",
    "        # Show retrieved images\n",
    "        for j, img_path in enumerate(retrieval_results[i]):\n",
    "            plt.subplot(1, 6, j+2)\n",
    "            plt.title(f\"Rank {j+1}\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.imshow(Image.open(img_path))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b44e2d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(query_labels, retrieval_results, gallery_labels):\n",
    "    correct = 0\n",
    "    for i, retrieved_imgs in enumerate(retrieval_results):\n",
    "        # Check if any retrieved image has the same label as query\n",
    "        query_label = query_labels[i]\n",
    "        retrieved_labels = [gallery_labels[os.path.basename(p)] for p in retrieved_imgs]\n",
    "        if query_label in retrieved_labels:\n",
    "            correct += 1\n",
    "    return correct / len(query_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d7d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "\n",
    "training_dir = os.path.join(DATA_DIR, \"training\")\n",
    "query_dir = os.path.join(DATA_DIR, \"test\", \"query\")\n",
    "gallery_dir = os.path.join(DATA_DIR, \"test\", \"gallery\")\n",
    "\n",
    "transform = preprocess\n",
    "\n",
    "train_dataset = ImageTextDataset(training_dir, transform=preprocess)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# fine_tune_clip(train_loader, model)\n",
    "\n",
    "gallery_features, gallery_paths = encode_images(gallery_dir, model, preprocess)\n",
    "query_features, query_paths = encode_images(query_dir, model, preprocess)\n",
    "\n",
    "retrieval_results = retrieve(query_features, gallery_features, gallery_paths, top_k=5)\n",
    "visualize_retrieval(query_paths, retrieval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
