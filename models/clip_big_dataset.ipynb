{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66e883d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc18fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "batch_size= 64\n",
    "FINE_TUNE = True  # Set to False to skip training\n",
    "TRAIN_LAST_LAYER_ONLY = False  # Set to False to fine-tune entire model\n",
    "epochs = 2\n",
    "lr = 1e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# image_size = (224, 224)\n",
    "# normalize_mean = [0.485, 0.456, 0.406]\n",
    "# normalize_std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae8d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, root_folder, transform):\n",
    "        self.transform = transform\n",
    "        self.img_paths = []\n",
    "        self.labels = []\n",
    "        # Walk through subfolders\n",
    "        for label_name in os.listdir(root_folder):\n",
    "            label_path = os.path.join(root_folder, label_name)\n",
    "            if os.path.isdir(label_path):\n",
    "                for img_file in os.listdir(label_path):\n",
    "                    self.img_paths.append(os.path.join(label_path, img_file))\n",
    "                    self.labels.append(label_name)  # folder name as label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.img_paths[idx]).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return image, label, os.path.basename(self.img_paths[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df31fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "if not FINE_TUNE:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "elif TRAIN_LAST_LAYER_ONLY:\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"proj\" in name or \"visual.proj\" in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "else:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "347cd201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_clip(train_loader, model, epochs=epochs, lr=lr):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for images, labels, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Prepare text tokens for labels\n",
    "            texts = clip.tokenize(labels).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            image_features = model.encode_image(images)\n",
    "            text_features = model.encode_text(texts)\n",
    "            \n",
    "            # Normalize embeddings\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "            \n",
    "            # Compute logits: image-text similarity\n",
    "            logits_per_image = image_features @ text_features.t()\n",
    "            logits_per_text = text_features @ image_features.t()\n",
    "            \n",
    "            # Labels are diagonal (correct pairs)\n",
    "            ground_truth = torch.arange(len(images), device=device)\n",
    "\n",
    "            # Compute contrastive loss (sum of two cross-entropy losses)\n",
    "            loss = (loss_fn(logits_per_image, ground_truth) + loss_fn(logits_per_text, ground_truth)) / 2\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c66d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(image_folder, model, preprocess):\n",
    "    image_paths = sorted([os.path.join(image_folder, f) for f in os.listdir(image_folder)])\n",
    "    images = [preprocess(Image.open(p).convert(\"RGB\")) for p in image_paths]\n",
    "    images = torch.stack(images).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = model.encode_image(images)\n",
    "        features /= features.norm(dim=-1, keepdim=True)\n",
    "    return features, image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0134d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query_features, gallery_features, gallery_paths, k):\n",
    "    similarities = query_features @ gallery_features.T  # cosine similarity matrix\n",
    "    topk_values, topk_indices = similarities.topk(k, dim=-1)\n",
    "\n",
    "    results = []\n",
    "    for i in range(query_features.shape[0]):\n",
    "        retrieved = [gallery_paths[idx] for idx in topk_indices[i].cpu().numpy()]\n",
    "        results.append(retrieved)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e743bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_retrieval(query_paths, retrieval_results):\n",
    "    for i, query_path in enumerate(query_paths):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Show query\n",
    "        plt.subplot(1, 6, 1)\n",
    "        plt.title(\"Query\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(Image.open(query_path))\n",
    "        \n",
    "        # Show retrieved images\n",
    "        for j, img_path in enumerate(retrieval_results[i]):\n",
    "            plt.subplot(1, 6, j+2)\n",
    "            plt.title(f\"Rank {j+1}\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.imshow(Image.open(img_path))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "608f2f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(query_labels, retrieval_results, gallery_labels):\n",
    "    correct = 0\n",
    "    for i, retrieved_imgs in enumerate(retrieval_results):\n",
    "        # Check if any retrieved image has the same label as query\n",
    "        query_label = query_labels[i]\n",
    "        retrieved_labels = [gallery_labels[os.path.basename(p)] for p in retrieved_imgs]\n",
    "        if query_label in retrieved_labels:\n",
    "            correct += 1\n",
    "    return correct / len(query_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a17147",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_DIR = \"/Users/terezasaskova/Desktop/ML/ml_project/ml-project-intro/data\"\n",
    "training_dir = os.path.join(DATA_DIR, \"train\")\n",
    "val_dir = os.path.join(DATA_DIR, \"val\")\n",
    "test_dir = os.path.join(DATA_DIR, \"test\")\n",
    "\n",
    "train_dataset = ImageTextDataset(training_dir, transform=preprocess)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "if FINE_TUNE:\n",
    "    fine_tune_clip(train_loader, model)\n",
    "\n",
    "# Encode validation or test images for retrieval or evaluation\n",
    "gallery_features, gallery_paths = encode_images(test_dir, model, preprocess)\n",
    "\n",
    "# You can also do the same for validation queries or test queries depending on your pipeline\n",
    "query_features, query_paths = encode_images(val_dir, model, preprocess)\n",
    "\n",
    "retrieval_results = retrieve(query_features, gallery_features, gallery_paths, k=k)\n",
    "visualize_retrieval(query_paths, retrieval_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
